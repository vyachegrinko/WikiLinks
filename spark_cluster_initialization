#set up aws_creds in bash profile:
nano .zshrc
export AWS_ACCESS_KEY_ID=
export AWS_SECRET_ACCESS_KEY=
source .zshrc
echo $AWS_SECRET_ACCESS_KEY
echo $AWS__ACCESS_KEY_ID

#update .pem file permissions
chmod 400 "/Users/datascientist/student_work/Sean/wikilinks/Galvanize_Sean_ONeal.pem"

#launch ec2 cluster with 6 slaves
/Users/datascientist/spark-1.6.1-bin-hadoop1/ec2/spark-ec2 -k Galvanize_Sean_ONeal -i ~/Users/datascientist/student_work/Sean/wikilinks/Galvanize_Sean_ONeal.pem -r us-east-1 -s 6 --copy-aws-credentials --ebs-vol-size=64 launch wiki_cluster

#-k: Name of your key-pair
#-i: Path to your (.pem) file
#-r: AWS region for your key-pair
#-s: The number of workers

#secure-copy over the install script to master node
scp -i /Users/datascientist/student_work/Sean/wikilinks/Galvanize_Sean_ONeal.pem student_work/Sean/wikilinks/install_scripts/install_these root@ec2-52-90-56-119.compute-1.amazonaws.com:/root/.

#log into master node
/Users/datascientist/spark-1.6.1-bin-hadoop1/ec2/spark-ec2 -k Galvanize_Sean_ONeal -i ~/Users/datascientist/student_work/Sean/wikilinks/Galvanize_Sean_ONeal.pem -r us-east-1 login wiki_cluster

#run install script
source install_these

#open new tmux session called 'notebook'
tmux new -s notebook

#open new python notebook
IPYTHON_OPTS="notebook --ip=0.0.0.0" /root/spark/bin/pyspark --executor-memory 4G --driver-memory 4G &

# go here to open notebook:
# http://ec2-52-90-56-119.compute-1.amazonaws.com:8888

#this command pauses the cluster:
/Users/datascientist/spark-1.6.1-bin-hadoop1/ec2/spark-ec2 -k Galvanize_Sean_ONeal -i ~/Users/datascientist/student_work/Sean/wikilinks/Galvanize_Sean_ONeal.pem -r us-east-1 stop wiki_cluster

#when paused, the cluster is only charged for storage space (not ec2 rental space)
#this command un-pauses the cluster back up again:
/Users/datascientist/spark-1.6.1-bin-hadoop1/ec2/spark-ec2 -k Galvanize_Sean_ONeal -i ~/Users/datascientist/student_work/Sean/wikilinks/Galvanize_Sean_ONeal.pem -r us-east-1 start wiki_cluster

#to connect to EC2 instance:
ssh -i "Galvanize_Sean_ONeal.pem" root@ec2-107-21-86-244.compute-1.amazonaws.com





# DOWNLOAD WIKI FROM AWS!!!
http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html#using-public-data-sets-launching-set

http://aws.amazon.com/datasets/wikipedia-xml-data/?tag=datasets%23keywords%23encyclopedic

#all wiki articles
wiki_rdd = sc.textFile('s3n://jyt109/wiki_articles')

ssc.sparkContext.hadoopConfiguration.set("fs.s3n.awsAccessKeyId",AKIAIH4RTAE6YQ3INLUQ)
ssc.sparkContext.hadoopConfiguration.set(X2FXpe0Ae4k2qdM+lfGg54Y+OaoECV2WpblUqJS+)
