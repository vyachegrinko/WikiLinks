degrees of separation
wiki page recommender
unconstructive comment identifier
cluster documents in some way, determine cluster meanings

STEPS (NOTES FOR FINAL PRESENTATION)
-create volume with all of english wiki articles (minus the talk pages, past revisions, and imagery): 500 GB
-since Spark reads in data 1 line at a time, xml is not an appropriate format. Unfortunately, when your dataset is 63GB, you can't easily transform the file...

it took ~20 minutes to perform a simple logic test for the whole 15 million pages

QUESTIONS FOR TIM:
why are there so many volumes?



Nikki: haaldeborgh@gmail.com

1 master, 18 slaves #
each slave has 2 cores and 7.5GB ram

to put each article on 1 line, the process took:
started at 4:45
ended at ?:??


Do Data Science after Hitler
discuss most important nodes (importance classified by how many paths crossed through that node on their way to Data Science/Hitler) in order to do this, I will have to consider that there can be multiple minimum distances for each node
mention that paths are directional (shortest path to Hitler will not necessarily be the same as path from Hitler)
